{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "# Utils\n",
    "class TextFileLoader:\n",
    "    def __init__(self, path: str, encoding: str = \"utf-8\"):\n",
    "        self.documents = []\n",
    "        self.path = path\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def load(self):\n",
    "        if os.path.isdir(self.path):\n",
    "            self.load_directory()\n",
    "        elif os.path.isfile(self.path) and self.path.endswith(\".txt\"):\n",
    "            self.load_file()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Provided path is neither a valid directory nor a .txt file.\"\n",
    "            )\n",
    "\n",
    "    def load_file(self):\n",
    "        with open(self.path, \"r\", encoding=self.encoding) as f:\n",
    "            self.documents.append(f.read())\n",
    "\n",
    "    def load_directory(self):\n",
    "        for root, _, files in os.walk(self.path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".txt\"):\n",
    "                    with open(\n",
    "                        os.path.join(root, file), \"r\", encoding=self.encoding\n",
    "                    ) as f:\n",
    "                        self.documents.append(f.read())\n",
    "\n",
    "    def load_documents(self):\n",
    "        self.load()\n",
    "        return self.documents\n",
    "\n",
    "\n",
    "class CharacterTextSplitter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "    ):\n",
    "        assert (\n",
    "            chunk_size > chunk_overlap\n",
    "        ), \"Chunk size must be greater than chunk overlap\"\n",
    "\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def split(self, text: str) -> List[str]:\n",
    "        # chunks = []\n",
    "        # for i in range(0, len(text), self.chunk_size - self.chunk_overlap):\n",
    "        #     chunks.append(text[i : i + self.chunk_size])\n",
    "        return text.split(\"\\n\")\n",
    "\n",
    "    def split_texts(self, texts: List[str]) -> List[str]:\n",
    "        chunks = []\n",
    "        for text in texts:\n",
    "            chunks.extend(self.split(text))\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector DB\n",
    "from gpt4all import Embed4All\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Callable\n",
    "import asyncio\n",
    "\n",
    "\n",
    "def cosine_similarity(vector_a: np.array, vector_b: np.array) -> float:\n",
    "    \"\"\"Computes the cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(vector_a, vector_b)\n",
    "    norm_a = np.linalg.norm(vector_a)\n",
    "    norm_b = np.linalg.norm(vector_b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self, embedding_model: Embed4All = None):\n",
    "        self.vectors = defaultdict(np.array)\n",
    "        self.embedding_model = embedding_model or Embed4All()\n",
    "        self.verbose = True\n",
    "\n",
    "    def insert(self, key: str, vector: np.array) -> None:\n",
    "        self.vectors[key] = vector\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query_vector: np.array,\n",
    "        k: int,\n",
    "        distance_measure: Callable = cosine_similarity,\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        if self.verbose:\n",
    "            print(\"Searching VectorDB\")\n",
    "        scores = [\n",
    "            (key, distance_measure(query_vector, vector))\n",
    "            for key, vector in self.vectors.items()\n",
    "        ]\n",
    "        return sorted(scores, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    def search_by_text(\n",
    "        self,\n",
    "        query_text: str,\n",
    "        k: int,\n",
    "        distance_measure: Callable = cosine_similarity,\n",
    "        return_as_text: bool = False,\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        if self.verbose:\n",
    "            print(f\"Embedding {query_text[:10]}...\")\n",
    "        query_vector = self.embedding_model.embed(query_text)\n",
    "        results = self.search(query_vector, k, distance_measure)\n",
    "        ratings = [(\" \".join(rating[0].split(\",\")[-2:]), rating[1]) for rating in results]\n",
    "        return [rating[0] for rating in ratings] if return_as_text else ratings\n",
    "\n",
    "    def retrieve_from_key(self, key: str) -> np.array:\n",
    "        return self.vectors.get(key, None)\n",
    "\n",
    "    async def abuild_from_list(self, list_of_text: List[str]) -> \"VectorDatabase\":\n",
    "        embeddings = []\n",
    "        for text in list_of_text:\n",
    "            embedding = self.embedding_model.embed(text)\n",
    "            embeddings.append(embedding)\n",
    "        for text, embedding in zip(list_of_text, embeddings):\n",
    "            self.insert(text, np.array(embedding))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_loader = TextFileLoader(\"./data/professionalism.txt\")\n",
    "documents = text_loader.load_documents()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter()\n",
    "split_documents = text_splitter.split_texts(documents)\n",
    "len(split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/cbrousseau/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin\n"
     ]
    }
   ],
   "source": [
    "vector_db = VectorDatabase()\n",
    "vector_db = asyncio.run(vector_db.abuild_from_list(split_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain.prompts.chat import SystemMessage, HumanMessagePromptTemplate\n",
    "\n",
    "RAQA_PROMPT_TEMPLATE = \"\"\"\n",
    "Use the provided context to rate the User Input from 1-100 for professionalism with 1 being extremely toxic and 100 being always professional.\n",
    "\n",
    "You may not answer the user's query unless there is specific context in the following text.\n",
    "\n",
    "The following context contains similar-looking messages, along with a rating and a short explanation for the rating.\n",
    "\n",
    "Please respond similarly, with a rating between 1 and 100, and a short explanation of why.\n",
    "\n",
    "If you do not know the answer, or cannot answer, please respond with \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Input:\n",
    "{user_query}\n",
    "\"\"\"\n",
    "\n",
    "# raqa_template = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         SystemMessage(\n",
    "#             content=(\n",
    "#                 \"\"\"Use the provided context to answer the user's query.\n",
    "\n",
    "#                 You may not answer the user's query unless there is specific context in the following text.\n",
    "\n",
    "#                 If you do not know the answer, or cannot answer, please respond with \"I don't know\".\n",
    "\n",
    "#                 Context:\n",
    "#                 {context}\n",
    "#                 \"\"\"\n",
    "#             )\n",
    "#         ),\n",
    "#         HumanMessagePromptTemplate.from_template(\n",
    "#             \"\"\"\n",
    "#             User Query:\n",
    "#             {user_query}\n",
    "#             \"\"\"\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "class RetrievalAugmentedQAPipeline:\n",
    "    def __init__(self, vector_db_retriever: VectorDatabase, template=None) -> None:\n",
    "        self.llm = LlamaCpp(\n",
    "                    model_path=\"./models/hermes-llongma-2-13b-8k.ggmlv3.q2_K.bin\",\n",
    "                    n_gpu_layers=0,\n",
    "                    n_batch=512,\n",
    "                    n_ctx=8000,\n",
    "                    verbose=True\n",
    "                    )\n",
    "        self.template = template\n",
    "        self.vector_db_retriever = vector_db_retriever\n",
    "        self.verbose = True\n",
    "\n",
    "    def run_pipeline(self, user_query: str) -> str:\n",
    "        if self.verbose:\n",
    "            print(f\"Searching VectorDB for {user_query[:10]}...\")\n",
    "        context_list = self.vector_db_retriever.search_by_text(user_query, k=2)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Gathering context...\")\n",
    "        context_prompt = \"\"\n",
    "        for context in context_list:\n",
    "            context_prompt += context[0] + \"\\n\"\n",
    "\n",
    "        formatted_prompt_template = PromptTemplate(input_variables=['context', 'user_query'], template=self.template)\n",
    "        chain = LLMChain(llm=self.llm, prompt=formatted_prompt_template)\n",
    "        if self.verbose:\n",
    "            print(\"Running Chain\")\n",
    "\n",
    "        return chain.run({\"context\": context_prompt, \"user_query\":user_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./models/hermes-llongma-2-13b-8k.ggmlv3.q2_K.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 8000\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 6912\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 10 (mostly Q2_K)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 6243.01 MB (+ 6250.00 MB per state)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "llama_new_context_with_model: kv self size  = 6250.00 MB\n"
     ]
    }
   ],
   "source": [
    "raqa_pipeline = RetrievalAugmentedQAPipeline(\n",
    "    vector_db_retriever=vector_db,\n",
    "    template=RAQA_PROMPT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching VectorDB for I am deepl...\n",
      "Embedding I am deepl...\n",
      "Searching VectorDB\n",
      "[('\\t30 \\tProbably a lie but even if not not appropriate for a business message.', 0.15730239846591354), ('\\t20 \\tMean comparison and political', 0.15413201272703336)]\n",
      "Gathering context...\n",
      "Running Chain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 19168.17 ms\n",
      "llama_print_timings:      sample time =    67.65 ms /    94 runs   (    0.72 ms per token,  1389.55 tokens per second)\n",
      "llama_print_timings: prompt eval time = 19168.14 ms /   191 tokens (  100.36 ms per token,     9.96 tokens per second)\n",
      "llama_print_timings:        eval time = 789847.98 ms /    93 runs   ( 8492.99 ms per token,     0.12 tokens per second)\n",
      "llama_print_timings:       total time = 809421.24 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIn response, I would like to express my discomfort with that statement.\\n\\nAlternative response: \"You are free to share your opinion on your blog but I have no idea what you think about it.\"\\n\\nResponse: \"I don\\'t know what you are talking about when you mention me. I guess I was shocked by the way you presented me as a target for criticism, which isn\\'t very professional of you.\"'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raqa_pipeline.run_pipeline(\"I am deeply unhappy with the way my boss behaves around me.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PerspecToolformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
