{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import slack_sdk\n",
    "import pandas as pd\n",
    "import gc\n",
    "#from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import sys\n",
    "#from st_aggrid import GridOptionsBuilder, AgGrid, GridUpdateMode, DataReturnMode\n",
    "\n",
    "\n",
    "## FROM PERSPECTIVE DOCUMENTATION\n",
    "\n",
    "# allowed test types\n",
    "allowed = [\"TOXICITY\",\n",
    "           \"SEVERE_TOXICITY\",\n",
    "           \"TOXICITY_FAST\",\n",
    "           \"ATTACK_ON_AUTHOR\",\n",
    "           \"ATTACK_ON_COMMENTER\",\n",
    "           \"INCOHERENT\",\n",
    "           \"INFLAMMATORY\",\n",
    "           \"OBSCENE\",\n",
    "           \"OFF_TOPIC\",\n",
    "           \"UNSUBSTANTIAL\",\n",
    "           \"LIKELY_TO_REJECT\",\n",
    "           \"INSULT\"]\n",
    "\n",
    "class Perspective(object):\n",
    "\n",
    "    base_url = \"https://commentanalyzer.googleapis.com/v1alpha1\"\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def score(self, text, tests=[\"TOXICITY\"], context=None, languages=None, do_not_store=False, token=None, text_type=None):\n",
    "        # data validation\n",
    "        # make sure it's a valid test\n",
    "        # TODO: see if an endpoint that has valid types exists\n",
    "        if isinstance(tests, str):\n",
    "            tests = [tests]\n",
    "        if not isinstance(tests, (list, dict)) or tests is None:\n",
    "            raise ValueError(\"Invalid list/dictionary provided for tests\")\n",
    "        if isinstance(tests, list):\n",
    "            new_data = {}\n",
    "            for test in tests:\n",
    "                new_data[test] = {}\n",
    "            tests = new_data\n",
    "        if text_type:\n",
    "            if text_type.lower() == \"html\":\n",
    "                text = remove_html(text)\n",
    "            elif text_type.lower() == \"md\":\n",
    "                text = remove_html(text, md=True)\n",
    "            else:\n",
    "                raise ValueError(\"{0} is not a valid text_type. Valid options are 'html' or 'md'\".format(str(text_type)))\n",
    "\n",
    "        for test in tests.keys():\n",
    "            if test not in allowed:\n",
    "                warnings.warn(\"{0} might not be accepted as a valid test.\".format(str(test)))\n",
    "            for key in tests[test].keys():\n",
    "                if key not in [\"scoreType\", \"scoreThreshhold\"]:\n",
    "                    raise ValueError(\"{0} is not a valid sub-property for {1}\".format(key, test))\n",
    "\n",
    "        # The API will only grade text less than 3k characters long\n",
    "        if len(text) > 3000:\n",
    "            # TODO: allow disassembly/reassembly of >3000char comments\n",
    "            warnings.warn(\"Perspective only allows 3000 character strings. Only the first 3000 characters will be sent for processing\")\n",
    "            text = text[:3000]\n",
    "        new_langs = []\n",
    "        if languages:\n",
    "            for language in languages:\n",
    "                language = language.lower()\n",
    "                if validate_language(language):\n",
    "                    new_langs.append(language)\n",
    "\n",
    "        # packaging data\n",
    "        url = Perspective.base_url + \"/comments:analyze\"\n",
    "        querystring = {\"key\": self.key}\n",
    "        payload_data = {\"comment\": {\"text\": text}, \"requestedAttributes\": {}}\n",
    "        for test in tests.keys():\n",
    "            payload_data[\"requestedAttributes\"][test] = tests[test]\n",
    "        if new_langs != None:\n",
    "            payload_data[\"languages\"] = new_langs\n",
    "        if do_not_store:\n",
    "            payload_data[\"doNotStore\"] = do_not_store\n",
    "        payload = json.dumps(payload_data)\n",
    "        headers = {'content-type': \"application/json\"}\n",
    "        response = requests.post(url,\n",
    "                            data=payload,\n",
    "                            headers=headers,\n",
    "                            params=querystring)\n",
    "        data = response.json()\n",
    "        if \"error\" in data.keys():\n",
    "            raise PerspectiveAPIException(data[\"error\"][\"message\"])\n",
    "        c = Comment(text, [], token)\n",
    "        base = data[\"attributeScores\"]\n",
    "        for test in tests.keys():\n",
    "            score = base[test][\"summaryScore\"][\"value\"]\n",
    "            score_type = base[test][\"summaryScore\"][\"type\"]\n",
    "            a = Attribute(test, [], score, score_type)\n",
    "            for span in base[test][\"spanScores\"]:\n",
    "                beginning = span[\"begin\"]\n",
    "                end = span[\"end\"]\n",
    "                score = span[\"score\"][\"value\"]\n",
    "                score_type = span[\"score\"][\"type\"]\n",
    "                s = Span(beginning, end, score, score_type, c)\n",
    "                a.spans.append(s)\n",
    "            c.attributes.append(a)\n",
    "        return c\n",
    "\n",
    "class Comment(object):\n",
    "    def __init__(self, text, attributes, token):\n",
    "        self.text = text\n",
    "        self.attributes = attributes\n",
    "        self.token = token\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if key.upper() not in allowed:\n",
    "            raise ValueError(\"value {0} does not exist\".format(key))\n",
    "        for attr in self.attributes:\n",
    "            if attr.name.lower() == key.lower():\n",
    "                return attr\n",
    "        raise ValueError(\"value {0} not found\".format(key))\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.text\n",
    "\n",
    "    def __repr__(self):\n",
    "        count = 0\n",
    "        num = 0\n",
    "        for attr in self.attributes:\n",
    "            count += attr.score\n",
    "            num += 1\n",
    "        return \"<({0}) {1}>\".format(str(count/num), self.text)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.attributes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "class Attribute(object):\n",
    "    def __init__(self, name, spans, score, score_type):\n",
    "        self.name = name\n",
    "        self.spans = spans\n",
    "        self.score = score\n",
    "        self.score_type = score_type\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.spans[index]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.spans)\n",
    "\n",
    "class Span(object):\n",
    "    def __init__(self, begin, end, score, score_type, comment):\n",
    "        self.begin = begin\n",
    "        self.end = end\n",
    "        self.score = score\n",
    "        self.score_type = score_type\n",
    "        self.comment = comment\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.comment.text[self.begin:self.end]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<({0}) {1}>\".format(self.score, self.comment.text[self.begin:self.end])\n",
    "\n",
    "class PerspectiveAPIException(Exception):\n",
    "    pass\n",
    "\n",
    "##### main function definition\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: python slack_toxicity_app.py YOUR_SLACK_API_KEY\")\n",
    "        return\n",
    "    \n",
    "    token_slack = sys.argv[1]\n",
    "\n",
    "    ### get slack data\n",
    "    # Create a client instance\n",
    "    client = slack_sdk.WebClient(token=token_slack)\n",
    "\n",
    "    # Get list of all direct message channels\n",
    "    dm_channels_response = client.conversations_list(types=\"im\")\n",
    "\n",
    "    # Prepare a dictionary to store all messages\n",
    "    all_messages = {}\n",
    "\n",
    "    # Iterate over each direct message channel\n",
    "    for channel in dm_channels_response[\"channels\"]:\n",
    "        # Get conversation history\n",
    "        history_response = client.conversations_history(channel=channel[\"id\"])\n",
    "\n",
    "        # Store messages\n",
    "        all_messages[channel[\"id\"]] = history_response[\"messages\"]\n",
    "\n",
    "    toxicity_scores = {}\n",
    "    txts = []\n",
    "    # progress_text = \"Slack Messages Downloading. Please wait.\"\n",
    "    # my_bar = st.progress(0, text=progress_text)\n",
    "\n",
    "    for channel_id, messages in all_messages.items():\n",
    "        for message in messages:\n",
    "            try:\n",
    "                text = message[\"text\"]\n",
    "                user = message[\"user\"]\n",
    "                #toxicity_score = perspective.get_toxicity_score(text)\n",
    "                timestamp = message[\"ts\"]\n",
    "                txts.append([timestamp,user,text])\n",
    "                #toxicity_scores[timestamp] = toxicity_score\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    df = pd.DataFrame(txts)\n",
    "    df.columns =  ['timestamp','user','text']\n",
    "    #self_user = df['user'].value_counts().idxmax()\n",
    "    #df = df[df.user == self_user]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    comments = df['text']\n",
    "\n",
    "    num_to_test = len(comments)\n",
    "\n",
    "    st.markdown(\"Getting scores for {num} messages.\".format(num = num_to_test))\n",
    "\n",
    "    #### run api\n",
    "    google_api_key = 'AIzaSyCDlnuintUJAi1HKa4nAScA52T1gbn9v8g'\n",
    "    client = Perspective(google_api_key)\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    toxicity_scores = []\n",
    "    insult_scores = []\n",
    "    obscenity_scores = []\n",
    "\n",
    "    #num_to_test = st.text_input(\"How many to run this on?\", len(comments))\n",
    "    #progress_text = \"Slack Messages Processing. Please wait.\"\n",
    "    #my_bar = st.progress(0, text=progress_text)\n",
    "\n",
    "\n",
    "    #start = time.time()\n",
    "    for i, comment in enumerate(comments[:num_to_test]):\n",
    "        #if detect(comment) == 'en':\n",
    "        #my_bar.progress(1/num_to_test, text=progress_text)\n",
    "        #current = time.time()\n",
    "        #time.sleep(.05) # limit API calls to 1 per second\n",
    "        try:\n",
    "            toxicity = client.score(comment, tests=[\"TOXICITY\", \"INSULT\", \"OBSCENE\"])\n",
    "            \n",
    "            #target = targets[i]\n",
    "            toxicity_scores.append(toxicity[\"TOXICITY\"].score)\n",
    "            insult_scores.append(toxicity[\"INSULT\"].score)\n",
    "            obscenity_scores.append(toxicity[\"OBSCENE\"].score)\n",
    "        except:\n",
    "            toxicity_scores.append(0)\n",
    "            insult_scores.append(0)\n",
    "            obscenity_scores.append(0)\n",
    "\n",
    "\n",
    "        df_eda = df[0:num_to_test]\n",
    "        df_eda['toxicity_scores'] = toxicity_scores\n",
    "        df_eda['insult_scores'] = insult_scores\n",
    "        df_eda['obscenity_scores'] = obscenity_scores\n",
    "        df_eda['timestamp'] = pd.to_datetime(df_eda['timestamp'],unit='s')\n",
    "\n",
    "        df_json = df_eda.to_json\n",
    "\n",
    "        return df_json\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
